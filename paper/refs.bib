
 % Les cinq métiers les plus menacés par l'intelligence artificielle
 @misc{demichelis_2018,
    title={Les Cinq métiers Les Plus menacés par l'intelligence artificielle},
    url={https://www.lesechos.fr/tech-medias/intelligence-artificielle/les-cinq-metiers-les-plus-menaces-par-lintelligence-artificielle-137080},
    journal={Les Echos},
    publisher={Les Echos},
    author={Demichelis, Remy},
    year={2018},
    month={Aug}
} 

% ResNet
% https://doi.org/10.48550/arxiv.1512.03385
@misc{ResNet,
  doi = {10.48550/ARXIV.1512.03385},
  url = {https://arxiv.org/abs/1512.03385},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Residual Learning for Image Recognition},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


% GA-UNet
% Kaur2021
@article{GA-UNet,
  doi = {10.1007/s00521-021-06134-z},
  url = {https://doi.org/10.1007/s00521-021-06134-z},
  year = {2021},
  month = jun,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {33},
  number = {21},
  pages = {14991--15025},
  author = {Amrita Kaur and Lakhwinder Kaur and Ashima Singh},
  title = {{GA}-{UNet}: {UNet}-based framework for segmentation of 2D and 3D medical images applicable on heterogeneous datasets},
  journal = {Neural Computing and Applications}
}

% U-Net++
% 10.1007/978-3-030-00889-5_1
@InProceedings{U-Net++,
author="Zhou, Zongwei
and Rahman Siddiquee, Md Mahfuzur
and Tajbakhsh, Nima
and Liang, Jianming",
editor="Stoyanov, Danail
and Taylor, Zeike
and Carneiro, Gustavo
and Syeda-Mahmood, Tanveer
and Martel, Anne
and Maier-Hein, Lena
and Tavares, Jo{\~a}o Manuel R.S.
and Bradley, Andrew
and Papa, Jo{\~a}o Paulo
and Belagiannis, Vasileios
and Nascimento, Jacinto C.
and Lu, Zhi
and Conjeti, Sailesh
and Moradi, Mehdi
and Greenspan, Hayit
and Madabhushi, Anant",
title="UNet++: A Nested U-Net Architecture for Medical Image Segmentation",
booktitle="Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="3--11",
abstract="In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.",
isbn="978-3-030-00889-5"
}

% ENet
% https://doi.org/10.48550/arxiv.1606.02147
@misc{ENet,
  doi = {10.48550/ARXIV.1606.02147},
  url = {https://arxiv.org/abs/1606.02147},
  author = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


% ENet cardiac segmentation
% 10.1007/978-3-319-59448-4_13
@InProceedings{ENet-cardiac-segmentation,
author="Lieman-Sifry, Jesse
and Le, Matthieu
and Lau, Felix
and Sall, Sean
and Golden, Daniel",
editor="Pop, Mihaela
and Wright, Graham A",
title="FastVentricle: Cardiac Segmentation with ENet",
booktitle="Functional Imaging and Modelling of the Heart",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="127--138",
abstract="Cardiac Magnetic Resonance (CMR) imaging is commonly used to assess cardiac structure and function. One disadvantage of CMR is that postprocessing of exams is tedious. Without automation, precise assessment of cardiac function via CMR typically requires an annotator to spend tens of minutes per case manually contouring ventricular structures. Automatic contouring can lower the required time per patient by generating contour suggestions that can be lightly modified by the annotator. Fully convolutional networks (FCNs), a variant of convolutional neural networks, have been used to rapidly advance the state-of-the-art in automated segmentation, which makes FCNs a natural choice for ventricular segmentation. However, FCNs are limited by their computational cost, which increases the monetary cost and degrades the user experience of production systems. To combat this shortcoming, we have developed the FastVentricle architecture, an FCN architecture for ventricular segmentation based on the recently developed ENet architecture. FastVentricle is 4{\$}{\$}{\backslash}times {\$}{\$}faster and runs with 6{\$}{\$}{\backslash}times {\$}{\$}less memory than the previous state-of-the-art ventricular segmentation architecture while still maintaining excellent clinical accuracy.",
isbn="978-3-319-59448-4"
}


% UNet
% 10.1007/978-3-319-24574-4_28
@InProceedings{UNet,
author="Ronneberger, Olaf
and Fischer, Philipp
and Brox, Thomas",
editor="Navab, Nassir
and Hornegger, Joachim
and Wells, William M.
and Frangi, Alejandro F.",
title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="234--241",
abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
isbn="978-3-319-24574-4"
}


% PSPNet
% https://doi.org/10.48550/arxiv.1612.01105
@misc{PSPNet,
  doi = {10.48550/ARXIV.1612.01105},
  url = {https://arxiv.org/abs/1612.01105},
  author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Pyramid Scene Parsing Network},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


% Focal-Loss
% https://doi.org/10.48550/arxiv.1708.02002
@misc{Focal-Loss,
  doi = {10.48550/ARXIV.1708.02002},
  url = {https://arxiv.org/abs/1708.02002},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Focal Loss for Dense Object Detection},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% multidepth_fusion
% 8642875
@ARTICLE{multidepth_fusion,
  author={Ye, Chengqin and Wang, Wei and Zhang, Shanzhuo and Wang, Kuanquan},
  journal={IEEE Access}, 
  title={Multi-Depth Fusion Network for Whole-Heart CT Image Segmentation}, 
  year={2019},
  volume={7},
  number={},
  pages={23421-23429},
  doi={10.1109/ACCESS.2019.2899635}
}